---
title: 'RAG with LangChain: Step-by-Step Practical Implementation'
slug: 'rag-langchain-practical-implementation'
publishedAt: '2025-01-15'
summary: 'Complete guide on how to implement Retrieval-Augmented Generation using LangChain, with real code examples and production best practices.'
translations:
  pt: 'rag-langchain-implementacao-pratica'
---

## Introduction to RAG

Retrieval-Augmented Generation (RAG) revolutionized how we build AI applications. During my experience developing intelligent chat systems, I discovered that RAG solves a crucial problem: how to provide specific and up-to-date context to LLMs without needing to retrain them.

In this article, I'll share my experience implementing RAG in production, the challenges I faced, and the solutions that worked.

## What is RAG and Why You Need It

RAG combines the best of two worlds:
- **Retrieval**: Searches for relevant information in a knowledge base
- **Generation**: Uses an LLM to generate responses based on that information

**Real problem I faced**: We needed to create an assistant that answered questions about technical documentation of over 1000 pages. Training a model from scratch would be expensive and time-consuming. RAG was the solution.

## System Architecture

Here's the architecture I implemented:

```
Document → Chunking → Embeddings → Vector DB → Retrieval → LLM → Response
```

### Main Components

1. **Document Loaders**: Load documents from various sources
2. **Text Splitters**: Divide documents into manageable chunks
3. **Embedding Models**: Convert text to vectors
4. **Vector Store**: Store and search vectors efficiently
5. **LLM**: Generate responses based on retrieved context

## Practical Implementation with LangChain

### Step 1: Installation and Setup

```bash
npm install langchain @langchain/openai @langchain/community
npm install faiss-node  # For local vector store
```

### Step 2: Loading and Processing Documents

```javascript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

// Load documents
const loader = new PDFLoader("technical-documentation.pdf");
const docs = await loader.load();

// Split into chunks
const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});

const chunks = await textSplitter.splitDocuments(docs);
console.log(\`Documents split into \${chunks.length} chunks\`);
```

**Why 1000 characters?** I tested various sizes (500, 1000, 1500, 2000). 1000 characters offered the best balance between context and precision for our use case.

**Why 200 overlap?** The overlap ensures that information appearing at chunk boundaries isn't lost.

### Step 3: Creating Embeddings and Vector Store

```javascript
import { OpenAIEmbeddings } from "@langchain/openai";
import { FaissStore } from "@langchain/community/vectorstores/faiss";

// Create embeddings
const embeddings = new OpenAIEmbeddings({
  modelName: "text-embedding-3-small", // Cheaper and faster
});

// Create and populate vector store
const vectorStore = await FaissStore.fromDocuments(
  chunks,
  embeddings
);

// Save for reuse
await vectorStore.save("./vectorstore");
console.log("Vector store created and saved!");
```

**Important lesson**: Creating embeddings is expensive. Always save your vector store so you don't need to reprocess everything.

### Step 4: Implementing Retrieval

```javascript
// Load existing vector store
const loadedVectorStore = await FaissStore.load(
  "./vectorstore",
  embeddings
);

// Create retriever
const retriever = loadedVectorStore.asRetriever({
  k: 4, // Number of documents to return
  searchType: "similarity",
});

// Test retrieval
const query = "How to configure authentication?";
const relevantDocs = await retriever.getRelevantDocuments(query);

console.log(\`Found \${relevantDocs.length} relevant documents\`);
relevantDocs.forEach((doc, i) => {
  console.log(\`\\nDocument \${i + 1}:\`);
  console.log(doc.pageContent.substring(0, 200) + "...");
});
```

### Step 5: Connecting with LLM

```javascript
import { ChatOpenAI } from "@langchain/openai";
import { RetrievalQAChain } from "langchain/chains";

const llm = new ChatOpenAI({
  modelName: "gpt-4-turbo-preview",
  temperature: 0, // More deterministic responses
});

const chain = RetrievalQAChain.fromLLM(llm, retriever);

const response = await chain.call({
  query: "How to configure authentication in the system?",
});

console.log("Response:", response.text);
```

## Optimizations and Best Practices

### 1. Strategic Chunking

I learned that not every document should be split the same way:

```javascript
// For code
const codeTextSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 500,
  chunkOverlap: 50,
  separators: ["\\n\\n", "\\n", " ", ""],
});

// For markdown
const mdTextSplitter = new MarkdownTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});
```

### 2. Enriched Metadata

Adding metadata significantly improves retrieval quality:

```javascript
const enhancedChunks = chunks.map((chunk, i) => ({
  ...chunk,
  metadata: {
    ...chunk.metadata,
    chunkIndex: i,
    source: "technical-documentation.pdf",
    section: extractSection(chunk.pageContent),
    timestamp: new Date().toISOString(),
  },
}));
```

### 3. Hybrid Search

I combined similarity search with keyword search for better results:

```javascript
const hybridRetriever = vectorStore.asRetriever({
  searchType: "mmr", // Maximum Marginal Relevance
  searchKwargs: {
    k: 6,
    fetchK: 20,
    lambda: 0.5,
  },
});
```

**MMR reduces redundancy** in returned results, giving more diverse responses.

## Challenges and Solutions

### Challenge 1: Embedding Cost

**Problem**: Processing thousands of documents became expensive quickly.

**Solution**:
- Used smaller embedding model (text-embedding-3-small)
- Implemented embedding cache
- Processed documents in batches

```javascript
// Process in batches
const batchSize = 100;
for (let i = 0; i < chunks.length; i += batchSize) {
  const batch = chunks.slice(i, i + batchSize);
  await vectorStore.addDocuments(batch);
  console.log(\`Processed batch \${i / batchSize + 1}\`);
}
```

### Challenge 2: Response Quality

**Problem**: Sometimes the LLM would invent information not present in documents.

**Solution**: Rigorous prompt engineering:

```javascript
const prompt = `You are a technical assistant that responds ONLY based on the provided context.

IMPORTANT RULES:
1. Use ONLY information from the context below
2. If information is not in the context, say "I didn't find that information in the documentation"
3. Cite the source when possible
4. Be precise and direct

Context: {context}

Question: {question}

Answer:`;
```

### Challenge 3: Performance

**Problem**: Retrieval too slow in production.

**Solution**: Migrated from FAISS to Pinecone (managed vector DB):

```javascript
import { PineconeStore } from "@langchain/pinecone";
import { Pinecone } from "@pinecone-database/pinecone";

const pinecone = new Pinecone();
const pineconeIndex = pinecone.index("documentation");

const vectorStore = await PineconeStore.fromDocuments(
  chunks,
  embeddings,
  { pineconeIndex }
);
```

**Result**: Latency reduced from ~2s to ~200ms.

## Monitoring and Metrics

I implemented detailed logging to track performance:

```javascript
const startTime = Date.now();

// Retrieval
const retrievalStart = Date.now();
const docs = await retriever.getRelevantDocuments(query);
const retrievalTime = Date.now() - retrievalStart;

// Generation
const generationStart = Date.now();
const response = await chain.call({ query });
const generationTime = Date.now() - generationStart;

console.log({
  totalTime: Date.now() - startTime,
  retrievalTime,
  generationTime,
  docsRetrieved: docs.length,
  query,
});
```

## Real Results

After implementing RAG:
- **Accuracy**: 92% correct responses (vs. 67% with pure LLM)
- **Speed**: ~500ms average response time
- **Cost**: 80% reduction compared to fine-tuning
- **Maintenance**: Document updates without retraining

## Conclusion and Next Steps

RAG completely transformed our application. The ability to add new knowledge without retraining models is invaluable.

### Next steps I recommend:

1. **Implement caching**: Common response cache reduces costs
2. **Add feedback loop**: Collect feedback to improve retrieval
3. **Experiment with reranking**: Use reranking models to improve results
4. **Monitor quality**: Implement response quality metrics

## Additional Resources

- [LangChain RAG Documentation](https://js.langchain.com/docs/use_cases/question_answering/)
- [Original RAG Paper](https://arxiv.org/abs/2005.11401)

I hope this guide helps you implement RAG efficiently. If you have questions, leave them in the comments!
